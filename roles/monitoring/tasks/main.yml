---
# Monitoring Role - Main Tasks with Cinder Volume Support
# Deploys Prometheus, Grafana, and full monitoring stack on master node
# Supports both Cinder partitioned volumes and local-path provisioner

- name: Wait for all cluster nodes to be ready
  become: yes
  become_user: ubuntu
  shell: |
    kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.conditions[?(@.type=="Ready")].status}{"\n"}{end}' | grep -c "True" || echo "0"
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  register: ready_nodes
  until: ready_nodes.stdout|int >= 1
  retries: 30
  delay: 20
  changed_when: false

- name: Wait for CoreDNS pods to be ready
  become: yes
  become_user: ubuntu
  shell: |
    kubectl wait --for=condition=Ready pods -l k8s-app=kube-dns -n kube-system --timeout=180s || true
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  retries: 2
  delay: 20
  ignore_errors: yes
  changed_when: false

- name: Check if Helm is installed
  shell: command -v helm
  register: helm_check
  changed_when: false
  failed_when: false

- name: Download and install Helm
  shell: |
    curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
  when: helm_check.rc != 0
  retries: 3
  delay: 10

- name: Verify Helm installation
  shell: helm version --short
  register: helm_version
  changed_when: false

- name: Display Helm version
  debug:
    var: helm_version.stdout

- name: Add Prometheus Community Helm repository
  become: yes
  become_user: ubuntu
  shell: |
    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  changed_when: false
  retries: 3
  delay: 10

- name: Update Helm repositories
  become: yes
  become_user: ubuntu
  shell: |
    helm repo update
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  changed_when: false
  retries: 3
  delay: 10

- name: Create monitoring namespace
  become: yes
  become_user: ubuntu
  shell: |
    kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  changed_when: false

- name: Check if using Cinder storage
  set_fact:
    using_cinder: "{{ use_cinder | default(false) }}"
    cinder_storage_path: "{{ storage_path | default('/mnt/k8s-storage') }}"
    storage_class_name: "{{ 'manual' if (use_cinder | default(false)) else 'local-path' }}"

- name: Display storage configuration
  debug:
    msg:
      - "Using Cinder: {{ using_cinder }}"
      - "Storage Path: {{ cinder_storage_path }}"
      - "StorageClass: {{ storage_class_name }}"

# === CINDER STORAGE SETUP ===
- name: Create Persistent Volumes for Cinder storage
  become: yes
  become_user: ubuntu
  shell: |
    cat <<EOF | kubectl apply -f -
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: prometheus-pv
    spec:
      capacity:
        storage: 15Gi
      accessModes:
        - ReadWriteOnce
      persistentVolumeReclaimPolicy: Retain
      storageClassName: manual
      hostPath:
        path: {{ cinder_storage_path }}/prometheus
        type: DirectoryOrCreate
      nodeAffinity:
        required:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - k8s-master
    ---
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: grafana-pv
    spec:
      capacity:
        storage: 5Gi
      accessModes:
        - ReadWriteOnce
      persistentVolumeReclaimPolicy: Retain
      storageClassName: manual
      hostPath:
        path: {{ cinder_storage_path }}/grafana
        type: DirectoryOrCreate
      nodeAffinity:
        required:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - k8s-master
    ---
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: alertmanager-pv
    spec:
      capacity:
        storage: 5Gi
      accessModes:
        - ReadWriteOnce
      persistentVolumeReclaimPolicy: Retain
      storageClassName: manual
      hostPath:
        path: {{ cinder_storage_path }}/alertmanager
        type: DirectoryOrCreate
      nodeAffinity:
        required:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - k8s-master
    EOF
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  when: using_cinder | bool
  changed_when: false

# === LOCAL-PATH PROVISIONER SETUP (fallback) ===
- name: Check if local-path storage provisioner exists
  become: yes
  become_user: ubuntu
  shell: |
    kubectl get deployment -n local-path-storage local-path-provisioner 2>/dev/null || echo "not_found"
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  register: storage_check
  changed_when: false
  when: not (using_cinder | bool)

- name: Install local-path storage provisioner
  become: yes
  become_user: ubuntu
  shell: |
    kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.28/deploy/local-path-storage.yaml
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  when:
    - not (using_cinder | bool)
    - "'not_found' in storage_check.stdout"
  retries: 3
  delay: 10

- name: Wait for local-path-provisioner to be ready
  become: yes
  become_user: ubuntu
  shell: |
    kubectl wait --for=condition=Ready pod -l app=local-path-provisioner -n local-path-storage --timeout=120s
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  when:
    - not (using_cinder | bool)
    - "'not_found' in storage_check.stdout"
  retries: 2
  delay: 10
  ignore_errors: yes

- name: Set local-path as default StorageClass
  become: yes
  become_user: ubuntu
  shell: |
    kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  when:
    - not (using_cinder | bool)
    - "'not_found' in storage_check.stdout"
  changed_when: false
  ignore_errors: yes

# === PROMETHEUS HELM VALUES ===
- name: Create custom values file for Prometheus stack
  copy:
    dest: /tmp/prometheus-values.yaml
    content: |
      # Prometheus Operator Configuration
      prometheusOperator:
        resources:
          limits:
            cpu: 200m
            memory: 256Mi
          requests:
            cpu: 100m
            memory: 128Mi
      
      # Prometheus Configuration
      prometheus:
        prometheusSpec:
          retention: 7d
          resources:
            requests:
              cpu: 200m
              memory: 512Mi
            limits:
              cpu: 500m
              memory: 1Gi
          storageSpec:
            volumeClaimTemplate:
              spec:
                storageClassName: {{ storage_class_name }}
                accessModes: ["ReadWriteOnce"]
                resources:
                  requests:
                    storage: 10Gi
      
      # Grafana Configuration
      grafana:
        enabled: true
        adminPassword: admin
        service:
          type: NodePort
          nodePort: 30080
        resources:
          limits:
            cpu: 200m
            memory: 256Mi
          requests:
            cpu: 100m
            memory: 128Mi
        persistence:
          enabled: true
          storageClassName: {{ storage_class_name }}
          size: 4Gi
      
      # AlertManager Configuration
      alertmanager:
        alertmanagerSpec:
          retention: 120h
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 200m
              memory: 256Mi
          storage:
            volumeClaimTemplate:
              spec:
                storageClassName: {{ storage_class_name }}
                accessModes: ["ReadWriteOnce"]
                resources:
                  requests:
                    storage: 4Gi
      
      # Node Exporter Configuration
      nodeExporter:
        enabled: true
      
      # Kube State Metrics Configuration
      kubeStateMetrics:
        enabled: true
      
      # Disable components not needed in small clusters
      kubeControllerManager:
        enabled: false
      kubeScheduler:
        enabled: false
      kubeProxy:
        enabled: true
      kubeEtcd:
        enabled: false
    owner: ubuntu
    group: ubuntu
    mode: '0644'

- name: Check if Prometheus stack is already installed
  become: yes
  become_user: ubuntu
  shell: |
    helm list -n monitoring | grep prometheus || echo "not_installed"
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  register: prometheus_installed
  changed_when: false

- name: Install Prometheus Stack using Helm
  become: yes
  become_user: ubuntu
  shell: |
    helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \
      --namespace monitoring \
      --create-namespace \
      --values /tmp/prometheus-values.yaml \
      --wait \
      --timeout 20m
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  when: "'not_installed' in prometheus_installed.stdout"
  register: helm_install
  async: 1800
  poll: 10
  retries: 2
  delay: 30
  ignore_errors: yes

- name: Wait for Prometheus Operator to be ready
  become: yes
  become_user: ubuntu
  shell: |
    kubectl wait --for=condition=Available --timeout=600s deployment/prometheus-kube-prometheus-operator -n monitoring
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  retries: 3
  delay: 30
  ignore_errors: yes

- name: Wait for Prometheus pods to be ready
  become: yes
  become_user: ubuntu
  shell: |
    kubectl wait --for=condition=Ready --timeout=600s pods -l app.kubernetes.io/name=prometheus -n monitoring
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  retries: 3
  delay: 30
  ignore_errors: yes

- name: Wait for Grafana pods to be ready
  become: yes
  become_user: ubuntu
  shell: |
    kubectl wait --for=condition=Ready --timeout=600s pods -l app.kubernetes.io/name=grafana -n monitoring
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  retries: 3
  delay: 30
  ignore_errors: yes

- name: Get Grafana admin password
  become: yes
  become_user: ubuntu
  shell: |
    kubectl get secret -n monitoring prometheus-grafana -o jsonpath="{.data.admin-password}" | base64 --decode
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  register: grafana_password
  changed_when: false

- name: Get Grafana service details
  become: yes
  become_user: ubuntu
  shell: |
    kubectl get svc -n monitoring prometheus-grafana -o jsonpath='{.spec.type}:{.spec.ports[0].nodePort}'
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  register: grafana_service
  changed_when: false

- name: Get Prometheus service details
  become: yes
  become_user: ubuntu
  shell: |
    kubectl get svc -n monitoring prometheus-kube-prometheus-prometheus
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  register: prometheus_service
  changed_when: false

- name: Display monitoring stack information
  debug:
    msg:
      - "========================================="
      - "Monitoring Stack Deployment Complete!"
      - "========================================="
      - "Storage Type: {{ 'Cinder Partitioned' if using_cinder else 'Local-Path' }}"
      - "Storage Class: {{ storage_class_name }}"
      - ""
      - "Grafana Access:"
      - "  - Service Type: {{ grafana_service.stdout.split(':')[0] }}"
      - "  - NodePort: {{ grafana_service.stdout.split(':')[1] }}"
      - "  - Username: admin"
      - "  - Password: {{ grafana_password.stdout }}"
      - ""
      - "Access Grafana:"
      - "  kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80"
      - "  Then browse to: http://localhost:3000"
      - ""
      - "Access Prometheus:"
      - "  kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090"
      - "  Then browse to: http://localhost:9090"
      - ""
      - "View all monitoring pods:"
      - "  kubectl get pods -n monitoring"
      - "========================================="

- name: Create monitoring dashboard ConfigMap
  become: yes
  become_user: ubuntu
  shell: |
    kubectl create configmap monitoring-info -n monitoring \
      --from-literal=grafana-password="{{ grafana_password.stdout }}" \
      --from-literal=storage-type="{{ 'Cinder' if using_cinder else 'Local-Path' }}" \
      --from-literal=access-info="Port-forward with: kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80" \
      --dry-run=client -o yaml | kubectl apply -f -
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  changed_when: false

- name: Verify all monitoring components are running
  become: yes
  become_user: ubuntu
  shell: |
    kubectl get pods -n monitoring -o wide
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  register: monitoring_pods
  changed_when: false

- name: Display monitoring pods status
  debug:
    var: monitoring_pods.stdout_lines

- name: Display storage information (Cinder only)
  become: yes
  become_user: ubuntu
  shell: |
    kubectl get pv,pvc -n monitoring
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  register: storage_info
  changed_when: false
  when: using_cinder | bool

- name: Show storage details
  debug:
    var: storage_info.stdout_lines
  when: using_cinder | bool

# === METRICS SERVER (required for HPA) ===
- name: Check if metrics-server is installed
  become: yes
  become_user: ubuntu
  shell: |
    kubectl get deployment metrics-server -n kube-system 2>/dev/null && echo 'installed' || echo 'not_installed'
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  register: metrics_server_check
  changed_when: false

- name: Install metrics-server
  become: yes
  become_user: ubuntu
  shell: |
    kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
    sleep 5
    kubectl patch deployment metrics-server -n kube-system --type='json' \
      -p='[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubelet-insecure-tls"}]'
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  when: "'not_installed' in metrics_server_check.stdout"
  retries: 3
  delay: 10
  ignore_errors: yes

- name: Wait for metrics-server to be ready
  become: yes
  become_user: ubuntu
  shell: |
    kubectl wait --for=condition=Available --timeout=120s deployment/metrics-server -n kube-system
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  retries: 3
  delay: 15
  ignore_errors: yes

# === HPA DEMO (Horizontal Pod Autoscaler) ===
- name: Deploy HPA demo application (php-apache)
  become: yes
  become_user: ubuntu
  shell: |
    cat <<EOF | kubectl apply -f -
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: php-apache
      namespace: default
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: php-apache
      template:
        metadata:
          labels:
            app: php-apache
        spec:
          containers:
          - name: php-apache
            image: registry.k8s.io/hpa-example
            ports:
            - containerPort: 80
            resources:
              requests:
                cpu: 200m
              limits:
                cpu: 500m
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: php-apache
      namespace: default
    spec:
      ports:
      - port: 80
      selector:
        app: php-apache
    ---
    apiVersion: autoscaling/v2
    kind: HorizontalPodAutoscaler
    metadata:
      name: php-apache-hpa
      namespace: default
    spec:
      scaleTargetRef:
        apiVersion: apps/v1
        kind: Deployment
        name: php-apache
      minReplicas: 1
      maxReplicas: 10
      metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 50
    EOF
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  changed_when: false

# === NGINX DEMO APP ===
- name: Deploy nginx demo application
  become: yes
  become_user: ubuntu
  shell: |
    cat <<EOF | kubectl apply -f -
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: nginx
      namespace: default
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: nginx:latest
            ports:
            - containerPort: 80
            resources:
              requests:
                cpu: 50m
                memory: 64Mi
              limits:
                cpu: 100m
                memory: 128Mi
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: nginx
      namespace: default
    spec:
      type: NodePort
      ports:
      - port: 80
        nodePort: 30088
      selector:
        app: nginx
    EOF
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  changed_when: false

# === ALERT RULES FOR DEMO ===
- name: Deploy custom PrometheusRule alerts
  become: yes
  become_user: ubuntu
  shell: |
    cat <<'EOF' | kubectl apply -f -
    apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      name: custom-alerts
      namespace: monitoring
      labels:
        release: prometheus
    spec:
      groups:
      - name: custom.rules
        rules:
        - alert: HighCPUUsage
          expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "High CPU usage detected"
            description: "CPU usage is above 80% for more than 2 minutes."
        - alert: HighMemoryUsage
          expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "High memory usage detected"
            description: "Memory usage is above 85% for more than 2 minutes."
        - alert: PodCrashLooping
          expr: rate(kube_pod_container_status_restarts_total[5m]) > 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Pod crash looping"
            description: "A pod is crash looping."
        - alert: HPAMaxedOut
          expr: kube_horizontalpodautoscaler_status_current_replicas == kube_horizontalpodautoscaler_spec_max_replicas
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "HPA at maximum replicas"
            description: "HPA is running at max capacity."
    EOF
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  changed_when: false
  ignore_errors: yes

# === AI MONITORING GRAFANA DASHBOARD ===
- name: Create AI monitoring dashboard JSON file
  copy:
    dest: /tmp/ai-dashboard.json
    content: |
      {
        "annotations": {"list": []},
        "editable": true,
        "panels": [
          {
            "title": "AI Anomaly Score - CPU",
            "type": "timeseries",
            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
            "targets": [{"expr": "abs(rate(node_cpu_seconds_total{mode=\"idle\"}[5m]) - avg_over_time(rate(node_cpu_seconds_total{mode=\"idle\"}[5m])[1h:])) / stddev_over_time(rate(node_cpu_seconds_total{mode=\"idle\"}[5m])[1h:])", "legendFormat": "Z-Score {{instance}}", "refId": "A"}],
            "fieldConfig": {"defaults": {"thresholds": {"mode": "absolute", "steps": [{"color": "green", "value": null}, {"color": "yellow", "value": 1.5}, {"color": "red", "value": 2.5}]}}}
          },
          {
            "title": "AI Anomaly Score - Memory",
            "type": "timeseries",
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0},
            "targets": [{"expr": "abs((node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes - avg_over_time(((node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes)[1h:])) / stddev_over_time(((node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes)[1h:])", "legendFormat": "Z-Score {{instance}}", "refId": "A"}],
            "fieldConfig": {"defaults": {"thresholds": {"mode": "absolute", "steps": [{"color": "green", "value": null}, {"color": "yellow", "value": 1.5}, {"color": "red", "value": 2.5}]}}}
          },
          {
            "title": "Pod Autoscaling Activity",
            "type": "timeseries",
            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8},
            "targets": [
              {"expr": "kube_horizontalpodautoscaler_status_current_replicas", "legendFormat": "Current {{horizontalpodautoscaler}}", "refId": "A"},
              {"expr": "kube_horizontalpodautoscaler_status_desired_replicas", "legendFormat": "Desired {{horizontalpodautoscaler}}", "refId": "B"}
            ]
          },
          {
            "title": "Network Anomaly Detection",
            "type": "timeseries",
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8},
            "targets": [{"expr": "abs(rate(node_network_receive_bytes_total[5m]) - avg_over_time(rate(node_network_receive_bytes_total[5m])[1h:])) / (stddev_over_time(rate(node_network_receive_bytes_total[5m])[1h:]) > 0)", "legendFormat": "Z-Score {{instance}} {{device}}", "refId": "A"}],
            "fieldConfig": {"defaults": {"thresholds": {"mode": "absolute", "steps": [{"color": "green", "value": null}, {"color": "yellow", "value": 2}, {"color": "red", "value": 3}]}}}
          },
          {
            "title": "Cluster Health Score (AI)",
            "type": "gauge",
            "gridPos": {"h": 8, "w": 8, "x": 0, "y": 16},
            "targets": [{"expr": "100 - (count(ALERTS{alertstate=\"firing\"}) or vector(0)) * 10 - (count(kube_pod_status_phase{phase=\"Failed\"}) or vector(0)) * 5", "refId": "A"}],
            "fieldConfig": {"defaults": {"min": 0, "max": 100, "thresholds": {"mode": "absolute", "steps": [{"color": "red", "value": null}, {"color": "yellow", "value": 50}, {"color": "green", "value": 80}]}}}
          },
          {
            "title": "Predictive CPU Capacity",
            "type": "timeseries",
            "gridPos": {"h": 8, "w": 8, "x": 8, "y": 16},
            "targets": [
              {"expr": "1 - avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m]))", "legendFormat": "Current CPU", "refId": "A"},
              {"expr": "predict_linear(avg(1 - rate(node_cpu_seconds_total{mode=\"idle\"}[5m]))[30m:1m], 3600)", "legendFormat": "Predicted (1h)", "refId": "B"}
            ]
          },
          {
            "title": "Active Alerts",
            "type": "table",
            "gridPos": {"h": 8, "w": 8, "x": 16, "y": 16},
            "targets": [{"expr": "ALERTS{alertstate=\"firing\"}", "format": "table", "instant": true, "refId": "A"}]
          }
        ],
        "schemaVersion": 38,
        "tags": ["ai", "monitoring", "anomaly"],
        "time": {"from": "now-1h", "to": "now"},
        "title": "AI Monitoring & Anomaly Detection",
        "uid": "ai-monitoring-dash"
      }
    owner: ubuntu
    group: ubuntu
    mode: '0644'

- name: Deploy AI monitoring dashboard ConfigMap
  become: yes
  become_user: ubuntu
  shell: |
    kubectl create configmap ai-monitoring-dashboard \
      --namespace monitoring \
      --from-file=ai-monitoring.json=/tmp/ai-dashboard.json \
      --dry-run=client -o yaml | \
    kubectl label --local -f - grafana_dashboard=1 -o yaml --dry-run=client | \
    kubectl apply -f -
  environment:
    KUBECONFIG: /home/ubuntu/.kube/config
  changed_when: false
  ignore_errors: yes

- name: Final deployment summary
  debug:
    msg:
      - "========================================="
      - "FULL DEPLOYMENT COMPLETE!"
      - "========================================="
      - "Kubernetes: Initialized with Calico CNI"
      - "Monitoring: Prometheus + Grafana + AlertManager"
      - "HPA: php-apache autoscaling demo active"
      - "Nginx: Demo app on NodePort 30088"
      - "AI Dashboard: Anomaly detection + predictions"
      - "Custom Alerts: CPU, Memory, CrashLoop, HPA"
      - "========================================="
